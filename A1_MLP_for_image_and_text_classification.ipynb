{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Importing libraries\n",
        "import keras\n",
        "import torch\n",
        "import warnings\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "warnings.simplefilter('ignore')"
      ],
      "metadata": {
        "id": "L24EsdSHtUiW"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "m75KRP98si32"
      },
      "outputs": [],
      "source": [
        "# Loading data\n",
        "cifar10 = keras.datasets.cifar10.load_data()\n",
        "\n",
        "imdb_reviews = keras.datasets.imdb.load_data(\n",
        "    skip_top=50,\n",
        "    num_words=5000,\n",
        "    start_char=1,\n",
        "    oov_char=2,\n",
        "    index_from=3\n",
        ")\n",
        "word_index   = keras.datasets.imdb.get_word_index()\n",
        "index_word   = {v + 3: k for k, v in word_index.items()}\n",
        "\n",
        "index_word[0] = '<PAD>'\n",
        "index_word[1] = '<START>'\n",
        "index_word[2] = '<OOV>'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CIFAR-10 Dataset\n",
        "\n",
        "The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.\n",
        "\n",
        "The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class.\n",
        "\n",
        "The classes are:\n",
        "\n",
        "| Label |\tDescription |\n",
        "|-------|-------------|\n",
        "| 0     |\tairplane    |\n",
        "| 1     |\tautomobile  |\n",
        "| 2     |\tbird        |\n",
        "| 3     |\tcat         |\n",
        "| 4     |\tdeer        |\n",
        "| 5     |\tdog         |\n",
        "| 6     |\tfrog        |\n",
        "| 7     |\thorse       |\n",
        "| 8     |\tship        |\n",
        "| 9     |\ttruck       |\n",
        "\n",
        "Source: https://www.cs.toronto.edu/~kriz/cifar.html"
      ],
      "metadata": {
        "id": "oAcVJvX9zytf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = cifar10\n",
        "x_train = x_train / 255\n",
        "x_test  = x_test / 255\n",
        "\n",
        "print(f'x_train shape: {x_train.shape}')\n",
        "print(f'y_train shape: {y_train.shape}')\n",
        "print(f'x_test shape: {x_test.shape}')\n",
        "print(f'y_test shape: {y_test.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wn4vEUnh1D_M",
        "outputId": "72d6cb6f-8e32-4312-e8e4-be145eaac5e3"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train shape: (50000, 32, 32, 3)\n",
            "y_train shape: (50000, 1)\n",
            "x_test shape: (10000, 32, 32, 3)\n",
            "y_test shape: (10000, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Keras MLP Image Classification"
      ],
      "metadata": {
        "id": "6u1T5_TU06Ae"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Building model\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Flatten(input_shape=(32, 32, 3)),\n",
        "    keras.layers.Dense(512, activation='relu'),\n",
        "    keras.layers.Dense(512, activation='relu'),\n",
        "    keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compiling model\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.SGD(learning_rate=0.01),\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
        "    metrics=[\n",
        "        keras.metrics.SparseCategoricalAccuracy()\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Fitting model\n",
        "model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    batch_size=32,\n",
        "    epochs=5,\n",
        "    validation_data=(x_test, y_test)\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSwMooZx1nJg",
        "outputId": "403b380c-3326-4093-bfd2-a190f01ae0a7"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 12ms/step - loss: 1.9575 - sparse_categorical_accuracy: 0.2943 - val_loss: 1.6978 - val_sparse_categorical_accuracy: 0.3964\n",
            "Epoch 2/5\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 13ms/step - loss: 1.6886 - sparse_categorical_accuracy: 0.4005 - val_loss: 1.6063 - val_sparse_categorical_accuracy: 0.4310\n",
            "Epoch 3/5\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 15ms/step - loss: 1.5980 - sparse_categorical_accuracy: 0.4351 - val_loss: 1.5611 - val_sparse_categorical_accuracy: 0.4492\n",
            "Epoch 4/5\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 15ms/step - loss: 1.5365 - sparse_categorical_accuracy: 0.4589 - val_loss: 1.5306 - val_sparse_categorical_accuracy: 0.4508\n",
            "Epoch 5/5\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 11ms/step - loss: 1.4844 - sparse_categorical_accuracy: 0.4759 - val_loss: 1.6258 - val_sparse_categorical_accuracy: 0.4203\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7c2482bcf6a0>"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pytorch MLP Image Classification"
      ],
      "metadata": {
        "id": "_pu2kMlF3FOa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = cifar10\n",
        "\n",
        "x_train = torch.from_numpy(x_train).float() / 255\n",
        "x_test  = torch.from_numpy(x_test).float() / 255\n",
        "\n",
        "y_train = torch.nn.functional.one_hot(torch.from_numpy(y_train).long(), num_classes=10).squeeze(1).float()\n",
        "y_test  = torch.nn.functional.one_hot(torch.from_numpy(y_test).long(), num_classes=10).squeeze(1).float()"
      ],
      "metadata": {
        "id": "zwsLrk9Q5hpT"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.nn.Sequential(\n",
        "    torch.nn.Flatten(),\n",
        "    torch.nn.Linear(32*32*3, 512),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(512, 512),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(512, 10),\n",
        "    torch.nn.Softmax(dim=1),\n",
        ")\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(5):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    train_acc  = 0\n",
        "    for batch in range(len(x_train) // 32):\n",
        "        x_batch = x_train[batch * 32: (batch + 1) * 32]\n",
        "        y_batch = y_train[batch * 32: (batch + 1) * 32]\n",
        "\n",
        "        y_pred = model(x_batch)\n",
        "        loss = loss_fn(y_pred, y_batch)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        train_acc  += (y_pred.argmax(dim=1) == y_batch.argmax(dim=1)).sum().item()\n",
        "\n",
        "    train_loss /= (batch + 1)\n",
        "    train_acc  /= len(x_train)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      test_loss = 0\n",
        "      test_acc  = 0\n",
        "      for batch in range(len(x_test) // 32):\n",
        "          x_batch = x_test[batch * 32: (batch + 1) * 32]\n",
        "          y_batch = y_test[batch * 32: (batch + 1) * 32]\n",
        "\n",
        "          y_pred = model(x_batch)\n",
        "          loss = loss_fn(y_pred, y_batch)\n",
        "\n",
        "          test_loss += loss.item()\n",
        "          test_acc  += (y_pred.argmax(dim=1) == y_batch.argmax(dim=1)).sum().item()\n",
        "\n",
        "      test_loss /= (batch + 1)\n",
        "      test_acc  /= len(x_test)\n",
        "\n",
        "    print(f'Epoch: {epoch + 1}')\n",
        "    print(f'Train loss: {train_loss:.4f} | Train acc: {train_acc:.4f}')\n",
        "    print(f'Test loss: {test_loss:.4f} | Test acc: {test_acc:.4f}')\n",
        "    print(\"-\" * 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jgyLnonh2VGn",
        "outputId": "8baeb3f7-ec38-4426-85d6-374cf67146ea"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1\n",
            "Train loss: 2.2946 | Train acc: 0.1115\n",
            "Test loss: 2.2831 | Test acc: 0.1259\n",
            "--------------------------------------------------\n",
            "Epoch: 2\n",
            "Train loss: 2.2628 | Train acc: 0.1642\n",
            "Test loss: 2.2396 | Test acc: 0.1974\n",
            "--------------------------------------------------\n",
            "Epoch: 3\n",
            "Train loss: 2.2165 | Train acc: 0.2446\n",
            "Test loss: 2.1958 | Test acc: 0.2645\n",
            "--------------------------------------------------\n",
            "Epoch: 4\n",
            "Train loss: 2.1857 | Train acc: 0.2727\n",
            "Test loss: 2.1766 | Test acc: 0.2764\n",
            "--------------------------------------------------\n",
            "Epoch: 5\n",
            "Train loss: 2.1714 | Train acc: 0.2854\n",
            "Test loss: 2.1653 | Test acc: 0.2885\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## IMDB Movie reviews\n",
        "\n",
        "This is a dataset of 25,000 movies reviews from IMDB, labeled by sentiment (positive/negative). Reviews have been preprocessed, and each review is encoded as a list of word indexes (integers). For convenience, words are indexed by overall frequency in the dataset.\n",
        "\n",
        "Source: https://ai.stanford.edu/~amaas/data/sentiment/"
      ],
      "metadata": {
        "id": "-nKk7LkYOO2Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Keras MLP Text Classification"
      ],
      "metadata": {
        "id": "vTzZUSHIO9-E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = imdb_reviews\n",
        "\n",
        "x_train = keras.utils.pad_sequences(x_train, maxlen=500)\n",
        "x_test  = keras.utils.pad_sequences(x_test,  maxlen=500)"
      ],
      "metadata": {
        "id": "PiT6AMOZbAOP"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Building model\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Embedding(5000, 32),\n",
        "    keras.layers.Flatten(),\n",
        "    keras.layers.Dense(512, activation='relu'),\n",
        "    keras.layers.Dense(512, activation='relu'),\n",
        "    keras.layers.Dense(2, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compiling model\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.SGD(learning_rate=0.01),\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
        "    metrics=[\n",
        "        keras.metrics.SparseCategoricalAccuracy()\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Fitting model\n",
        "model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    batch_size=32,\n",
        "    epochs=5,\n",
        "    validation_data=(x_test, y_test)\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "unV1qX15dTYQ",
        "outputId": "fa0a5b27-bca2-48b6-fc0f-b636e8d4cfca"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 99ms/step - loss: 0.6932 - sparse_categorical_accuracy: 0.5071 - val_loss: 0.6925 - val_sparse_categorical_accuracy: 0.5154\n",
            "Epoch 2/5\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 84ms/step - loss: 0.6916 - sparse_categorical_accuracy: 0.5228 - val_loss: 0.6923 - val_sparse_categorical_accuracy: 0.5169\n",
            "Epoch 3/5\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 85ms/step - loss: 0.6911 - sparse_categorical_accuracy: 0.5227 - val_loss: 0.6920 - val_sparse_categorical_accuracy: 0.5157\n",
            "Epoch 4/5\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 83ms/step - loss: 0.6898 - sparse_categorical_accuracy: 0.5324 - val_loss: 0.6919 - val_sparse_categorical_accuracy: 0.5155\n",
            "Epoch 5/5\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 85ms/step - loss: 0.6882 - sparse_categorical_accuracy: 0.5416 - val_loss: 0.6914 - val_sparse_categorical_accuracy: 0.5200\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x78bb8adad3f0>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pytorch MLP Text Classification"
      ],
      "metadata": {
        "id": "LNGfta1yd7X-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = imdb_reviews\n",
        "\n",
        "x_train = torch.from_numpy(keras.utils.pad_sequences(x_train, maxlen=500))\n",
        "x_test  = torch.from_numpy(keras.utils.pad_sequences(x_test,  maxlen=500))\n",
        "\n",
        "y_train = torch.nn.functional.one_hot(torch.from_numpy(y_train).long(), num_classes=2).float()\n",
        "y_test  = torch.nn.functional.one_hot(torch.from_numpy(y_test).long(), num_classes=2).float()"
      ],
      "metadata": {
        "id": "ynh_R8ZkdUbJ"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.nn.Sequential(\n",
        "    torch.nn.Embedding(5000, 32),\n",
        "    torch.nn.Flatten(),\n",
        "    torch.nn.Linear(16000, 512),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(512, 512),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(512, 2),\n",
        "    torch.nn.Softmax(dim=1),\n",
        ")\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(5):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    train_acc  = 0\n",
        "    for batch in range(len(x_train) // 32):\n",
        "        x_batch = x_train[batch * 32: (batch + 1) * 32]\n",
        "        y_batch = y_train[batch * 32: (batch + 1) * 32]\n",
        "\n",
        "        y_pred = model(x_batch)\n",
        "        loss = loss_fn(y_pred, y_batch)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        train_acc  += (y_pred.argmax(dim=1) == y_batch.argmax(dim=1)).sum().item()\n",
        "\n",
        "    train_loss /= (batch + 1)\n",
        "    train_acc  /= len(x_train)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      test_loss = 0\n",
        "      test_acc  = 0\n",
        "      for batch in range(len(x_test) // 32):\n",
        "          x_batch = x_test[batch * 32: (batch + 1) * 32]\n",
        "          y_batch = y_test[batch * 32: (batch + 1) * 32]\n",
        "\n",
        "          y_pred = model(x_batch)\n",
        "          loss = loss_fn(y_pred, y_batch)\n",
        "\n",
        "          test_loss += loss.item()\n",
        "          test_acc  += (y_pred.argmax(dim=1) == y_batch.argmax(dim=1)).sum().item()\n",
        "\n",
        "      test_loss /= (batch + 1)\n",
        "      test_acc  /= len(x_test)\n",
        "\n",
        "    print(f'Epoch: {epoch + 1}')\n",
        "    print(f'Train loss: {train_loss:.4f} | Train acc: {train_acc:.4f}')\n",
        "    print(f'Test loss: {test_loss:.4f} | Test acc: {test_acc:.4f}')\n",
        "    print(\"-\" * 50)"
      ],
      "metadata": {
        "id": "YDSZOEkBePlC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RWFAhSPZg_4x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}